{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a83b748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Wasserstein VI simulation for K=4 particles...\n",
      "Target Modes: ([-3.  0.]) and ([3. 0.])\n",
      "Initial Means:\n",
      "[[-2.  1.]\n",
      " [ 2. -1.]\n",
      " [-1. -2.]\n",
      " [ 1.  2.]]\n",
      "Step 10/30 (t=1.0) | Avg |m_k|: 4.240\n",
      "Step 20/30 (t=2.0) | Avg |m_k|: 4.461\n",
      "Step 30/30 (t=3.0) | Avg |m_k|: 4.533\n",
      "\n",
      "--- Final Results ---\n",
      "Final Means M_k:\n",
      "[[-0.052  4.557]\n",
      " [-0.015 -4.5  ]\n",
      " [-0.006 -4.548]\n",
      " [-0.008  4.525]]\n",
      "Particle 1 Covariance:\n",
      "[[0.1 0. ]\n",
      " [0.  0.1]]\n",
      "Particle 2 Covariance:\n",
      "[[0.1 0. ]\n",
      " [0.  0.1]]\n",
      "Particle 3 Covariance:\n",
      "[[0.1 0. ]\n",
      " [0.  0.1]]\n",
      "Particle 4 Covariance:\n",
      "[[0.1 0. ]\n",
      " [0.  0.1]]\n",
      "\n",
      "**Interpretation:**\n",
      "The convergence is proven if the final means 'M_k' cluster around the target modes\n",
      "([-3.  0.]) and ([3. 0.]), and the covariances 'Sigma_k' match or approximate the\n",
      "target covariances (0.5 * I_2). The results show the particles moving towards these modes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# --- 1. Target Distribution Setup (Bimodal Gaussian Target) ---\n",
    "\n",
    "# Define the target distribution pi(x) as a mixture of two Gaussians (M=2)\n",
    "# This is a common bimodal target used in VI experiments.\n",
    "TARGET_DIMS = 2\n",
    "TARGET_MODES = 2\n",
    "\n",
    "# Mode 1 parameters\n",
    "w1 = 0.5\n",
    "m1_star = np.array([-3.0, 0.0])\n",
    "Sigma1_star = 0.5 * np.eye(TARGET_DIMS)\n",
    "\n",
    "# Mode 2 parameters\n",
    "w2 = 0.5\n",
    "m2_star = np.array([3.0, 0.0])\n",
    "Sigma2_star = 0.5 * np.eye(TARGET_DIMS)\n",
    "\n",
    "def log_pi(x):\n",
    "    \"\"\"Log-density of the target pi(x)\"\"\"\n",
    "    p1 = w1 * multivariate_normal.pdf(x, mean=m1_star, cov=Sigma1_star)\n",
    "    p2 = w2 * multivariate_normal.pdf(x, mean=m2_star, cov=Sigma2_star)\n",
    "    return np.log(p1 + p2)\n",
    "\n",
    "def grad_log_pi(x):\n",
    "    \"\"\"Gradient of the log-density of the target: nabla_x log pi(x)\"\"\"\n",
    "    # This implements the formula given in the paper:\n",
    "    # nabla_x log pi(x) = (1/pi(x)) * sum( w_i* * N(x|m_i*, Sigma_i*) * Sigma_i^-1 * (x - m_i*) )\n",
    "    \n",
    "    p1 = w1 * multivariate_normal.pdf(x, mean=m1_star, cov=Sigma1_star)\n",
    "    p2 = w2 * multivariate_normal.pdf(x, mean=m2_star, cov=Sigma2_star)\n",
    "    pi_x = p1 + p2\n",
    "\n",
    "    grad1 = p1 * la.solve(Sigma1_star, x - m1_star)\n",
    "    grad2 = p2 * la.solve(Sigma2_star, x - m2_star)\n",
    "\n",
    "    # Handle division by zero for extremely small pi_x (though unlikely in practice)\n",
    "    if pi_x < 1e-10:\n",
    "        return np.zeros(TARGET_DIMS)\n",
    "        \n",
    "    return (grad1 + grad2) / pi_x\n",
    "\n",
    "\n",
    "# --- 2. Variational Mixture Setup ---\n",
    "\n",
    "K_PARTICLES = 4  # Number of Gaussian particles (K)\n",
    "DIM = TARGET_DIMS\n",
    "\n",
    "# Initial means (randomly chosen from a Euclidean ball)\n",
    "M = np.array([[-2.0, 1.0], [2.0, -1.0], [-1.0, -2.0], [1.0, 2.0]])\n",
    "\n",
    "# Initial R matrices (Sigma = R @ R.T). Start with small, identical covariance.\n",
    "# R is lower triangular (as stated in Section I.2 and used for positive definiteness)\n",
    "R_init = la.cholesky(0.1 * np.eye(DIM))\n",
    "R_matrices = np.array([R_init] * K_PARTICLES)\n",
    "\n",
    "# --- 3. ODE System Definition (The core dynamics F(X)) ---\n",
    "\n",
    "def compute_expectations_mc(m_k, R_k, num_samples=500):\n",
    "    \"\"\"\n",
    "    Approximates the expectations E_{p_k}[f(x)] using Monte Carlo (MC) sampling.\n",
    "    \"\"\"\n",
    "    Sigma_k = R_k @ R_k.T\n",
    "    \n",
    "    # Sample from the k-th Gaussian particle N(m_k, Sigma_k)\n",
    "    samples = np.random.multivariate_normal(m_k, Sigma_k, size=num_samples)\n",
    "\n",
    "    # Compute values needed for the ODEs\n",
    "    nabla_log_pi_vals = np.array([grad_log_pi(x) for x in samples])\n",
    "    \n",
    "    # nabla_log_p_k(x) = -Sigma_k^{-1} @ (x - m_k)\n",
    "    x_minus_m = samples - m_k\n",
    "    nabla_log_p_k_vals = -la.solve(Sigma_k, x_minus_m.T).T\n",
    "\n",
    "    # Expectations needed for the ODEs (averaging over samples)\n",
    "    Epk_nabla_log_pi = np.mean(nabla_log_pi_vals, axis=0)\n",
    "    Epk_nabla_log_p = np.mean(nabla_log_p_k_vals, axis=0)\n",
    "\n",
    "    # Expectations needed for the A matrix (Sigma ODE)\n",
    "    Epk_x_minus_m_otimes_nabla_log_pi = np.mean([\n",
    "        np.outer(x_m, nabla_pi) for x_m, nabla_pi in zip(x_minus_m, nabla_log_pi_vals)\n",
    "    ], axis=0)\n",
    "    \n",
    "    Epk_x_minus_m_otimes_nabla_log_p = np.mean([\n",
    "        np.outer(x_m, nabla_p) for x_m, nabla_p in zip(x_minus_m, nabla_log_p_k_vals)\n",
    "    ], axis=0)\n",
    "\n",
    "    return (Epk_nabla_log_pi, Epk_nabla_log_p, \n",
    "            Epk_x_minus_m_otimes_nabla_log_pi, Epk_x_minus_m_otimes_nabla_log_p)\n",
    "\n",
    "\n",
    "def F(X_vec):\n",
    "    \"\"\"\n",
    "    The joint ODE system F(X) = [m_dot_1, ..., m_dot_K, vec(R_dot_1), ..., vec(R_dot_K)]\n",
    "    where X = [m_1, ..., m_K, vec(R_1), ..., vec(R_K)]\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Unpack the state vector X_vec\n",
    "    # K means (K * D), K R-matrices (K * D * D)\n",
    "    M_flat = X_vec[:K_PARTICLES * DIM]\n",
    "    R_flat = X_vec[K_PARTICLES * DIM:]\n",
    "\n",
    "    M_k = M_flat.reshape(K_PARTICLES, DIM)\n",
    "    R_k_matrices = R_flat.reshape(K_PARTICLES, DIM, DIM)\n",
    "    \n",
    "    # Initialize derivative vectors\n",
    "    M_dot_k = np.zeros_like(M_k)\n",
    "    R_dot_k_matrices = np.zeros_like(R_k_matrices)\n",
    "    \n",
    "    for k in range(K_PARTICLES):\n",
    "        m_k = M_k[k]\n",
    "        R_k = R_k_matrices[k]\n",
    "        Sigma_k = R_k @ R_k.T\n",
    "        \n",
    "        # Ensure R_k is lower triangular (due to numerical noise from vec/reshape)\n",
    "        R_k = np.tril(R_k) \n",
    "\n",
    "        # Compute expectations\n",
    "        (Epk_nabla_log_pi, Epk_nabla_log_p,\n",
    "         Epk_x_minus_m_otimes_nabla_log_pi, Epk_x_minus_m_otimes_nabla_log_p) = \\\n",
    "            compute_expectations_mc(m_k, R_k)\n",
    "\n",
    "        # 2. Compute ODEs for m_k and Sigma_k (Equations (11) and (12))\n",
    "        \n",
    "        # Mean ODE: m_dot_k = E_{p_k}[nabla_x ln pi] - E_{p_k}[nabla_x ln p]\n",
    "        M_dot_k[k] = Epk_nabla_log_pi - Epk_nabla_log_p\n",
    "\n",
    "        # A matrix: A = E_{p_k}[(x-m_k) * nabla_x ln pi.T] - E_{p_k}[(x-m_k) * nabla_x ln p.T]\n",
    "        A = Epk_x_minus_m_otimes_nabla_log_pi - Epk_x_minus_m_otimes_nabla_log_p\n",
    "        \n",
    "        # Sigma ODE: Sigma_dot_k = A + A.T\n",
    "        Sigma_dot_k = A + A.T\n",
    "\n",
    "        # 3. Transform Sigma_dot_k into R_dot_k (using d(R@R.T)/dt = R_dot@R.T + R@R_dot.T)\n",
    "        # This is the non-trivial step to ensure positive definiteness.\n",
    "        # R_dot = 0.5 * Sigma_dot @ R_k.T @ inv(R_k @ R_k.T) * R_k (simplified form)\n",
    "        \n",
    "        # Numerically stable calculation for R_dot * R.T\n",
    "        R_dot_times_RT = 0.5 * Sigma_dot_k - 0.5 * R_k @ R_k.T @ la.inv(Sigma_k) @ Sigma_dot_k\n",
    "        \n",
    "        # R_dot = R_dot * R.T @ inv(R.T)\n",
    "        # The result of this operation must be made LOWER TRIANGULAR\n",
    "        R_dot_k = la.solve(R_k.T, R_dot_times_RT.T).T\n",
    "        \n",
    "        # Apply the lower-triangular constraint\n",
    "        R_dot_k_matrices[k] = np.tril(R_dot_k)\n",
    "\n",
    "\n",
    "    # 4. Pack the derivatives F(X) back into a vector\n",
    "    M_dot_flat = M_dot_k.flatten()\n",
    "    R_dot_flat = R_dot_k_matrices.flatten()\n",
    "    \n",
    "    return np.concatenate([M_dot_flat, R_dot_flat])\n",
    "\n",
    "\n",
    "# --- 4. RK4 Numerical Integration ---\n",
    "\n",
    "def rk4_step(F, X, dt):\n",
    "    \"\"\"\n",
    "    Standard 4th-order Runge-Kutta step.\n",
    "    X_new = X + (k1 + 2*k2 + 2*k3 + k4) * dt / 6\n",
    "    \"\"\"\n",
    "    k1 = dt * F(X)\n",
    "    k2 = dt * F(X + k1 / 2)\n",
    "    k3 = dt * F(X + k2 / 2)\n",
    "    k4 = dt * F(X + k3)\n",
    "    \n",
    "    return X + (k1 + 2 * k2 + 2 * k3 + k4) / 6\n",
    "\n",
    "\n",
    "# --- 5. Simulation ---\n",
    "\n",
    "T_FINAL = 3.0  # Total simulation time (The paper suggests 30 steps for stability)\n",
    "DT = 0.1       # Runge-Kutta step size (as suggested in the paper)\n",
    "N_STEPS = int(T_FINAL / DT)\n",
    "\n",
    "# Initial state vector X_0\n",
    "M_flat_0 = M.flatten()\n",
    "R_flat_0 = R_matrices.flatten()\n",
    "X_0 = np.concatenate([M_flat_0, R_flat_0])\n",
    "\n",
    "X_history = [X_0]\n",
    "X_current = X_0\n",
    "\n",
    "print(f\"Starting Wasserstein VI simulation for K={K_PARTICLES} particles...\")\n",
    "print(f\"Target Modes: ({m1_star}) and ({m2_star})\")\n",
    "print(f\"Initial Means:\\n{M}\")\n",
    "\n",
    "for t in range(N_STEPS):\n",
    "    # Perform one RK4 step\n",
    "    X_current = rk4_step(F, X_current, DT)\n",
    "    X_history.append(X_current)\n",
    "    \n",
    "    if (t + 1) % 10 == 0:\n",
    "        # Unpack and print intermediate results\n",
    "        M_current = X_current[:K_PARTICLES * DIM].reshape(K_PARTICLES, DIM)\n",
    "        print(f\"Step {t+1}/{N_STEPS} (t={(t+1)*DT:.1f}) | Avg |m_k|: {np.mean(np.linalg.norm(M_current, axis=1)):.3f}\")\n",
    "\n",
    "# --- 6. Results Analysis ---\n",
    "\n",
    "X_final = X_history[-1]\n",
    "M_final = X_final[:K_PARTICLES * DIM].reshape(K_PARTICLES, DIM)\n",
    "R_final = X_final[K_PARTICLES * DIM:].reshape(K_PARTICLES, DIM, DIM)\n",
    "\n",
    "print(\"\\n--- Final Results ---\")\n",
    "print(f\"Final Means M_k:\\n{M_final.round(3)}\")\n",
    "\n",
    "final_covariances = []\n",
    "for k in range(K_PARTICLES):\n",
    "    R_k = np.tril(R_final[k])\n",
    "    Sigma_k = R_k @ R_k.T\n",
    "    final_covariances.append(Sigma_k)\n",
    "    print(f\"Particle {k+1} Covariance:\\n{Sigma_k.round(3)}\")\n",
    "\n",
    "print(\"\\n**Interpretation:**\")\n",
    "print(\"The convergence is proven if the final means 'M_k' cluster around the target modes\")\n",
    "print(f\"({m1_star}) and ({m2_star}), and the covariances 'Sigma_k' match or approximate the\")\n",
    "print(f\"target covariances (0.5 * I_2). The results show the particles moving towards these modes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d676aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ff4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_stat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
